{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b63d815",
   "metadata": {},
   "source": [
    "# Assignment 3: Motion AutoEncoder\n",
    "\n",
    "In this assignment, you will implement and train an autoencoder for motion generation using the CMU-Mocap dataset. You will develop a simplified version of a motion manifold system capable of learning from motion capture data, synthesizing new movements, and enabling basic motion editing techniques.\n",
    "\n",
    "Please read the following two papers as they are your reference for your implementation:\n",
    "* Learning Motion Manifolds with Convolutional Autoencoders, https://www.ipab.inf.ed.ac.uk/cgvu/motioncnn.pdf\n",
    "* A Deep Learning Framework for Character Motion Synthesis and Editing, https://www.ipab.inf.ed.ac.uk/cgvu/motionsynthesis.pdf\n",
    "\n",
    "To help you understand motion data representation, here are some valuable resources:\n",
    "\n",
    "* **CMU Mocap dataset**: https://mocap.cs.cmu.edu/\n",
    "  * The raw data format may be challenging to interpret directly. For easier use, download the BVH format (which our dataloader utilizes): https://github.com/una-dinosauria/cmu-mocap\n",
    "  * Blender (https://www.blender.org/) can be used to visualize the downloaded BVH files. This will provide a clearer understanding of the human skeleton structure used in the dataset and allow you to visualize the raw motion sequences.\n",
    "* **Additional reference** (optional): For a deeper exploration of human models, you can examine more motion data in FBX format at: https://www.mixamo.com/#/?page=1&type=Motion%2CMotionPack\n",
    "  * Note that Mixamo uses a different skeletal structure than our dataset, but it includes human mesh deformation with motion, which BVH data doesn't provide.\n",
    "\n",
    "\n",
    "This assignment includes substantial starter code in separate Python files to help you get started. Your tasks are to:\n",
    "\n",
    "* Complete all sections marked with `TODO` comments. Feel free to add code or restructure functions to improve your implementation's flexibility during training.\n",
    "* Present your results by embedding videos and images in this notebook. You should also answer all questions and complete the write-up sections in this notebook.\n",
    "* Include all necessary video files, images, and code files in your submission for proper evaluation. **Important**: Do NOT submit model checkpoints as these will unnecessarily increase your submission size.\n",
    "\n",
    "**Please reserve enough time for this assignment given the potential amount of time for training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d6697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For you to add video to your submission.\n",
    "from IPython.display import Video\n",
    "Video('./vid/sample_0.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f45bce",
   "metadata": {},
   "source": [
    "## Part 1: Familiarize Yourself with the Data (10 pt)\n",
    "\n",
    "Unlike previous assignments where standard dataloaders were readily available in existing libraries, this assignment requires implementing a custom dataloader for the CMU-Mocap dataset.\n",
    "\n",
    "We've provided most of the dataloader implementation in `dataloader.py` and included a pre-processed version of the necessary data in `cmu-mocap/cache` (https://utexas.box.com/v/cmu-mocap-cache). Please download this data and place the `cmu-mocap` folder in your current working directory. Alternatively, you can download the BVH files from the links provided earlier and generate your own pre-processed data.\n",
    "\n",
    "For this part, you need to implement data normalization in the dataloader. There are two sets of TODOs in the file:\n",
    "\n",
    "1. First, compute appropriate statistics for normalizing your data based on the entire dataset\n",
    "2. Second, implement the normalization procedure in the `__getitem__` function where the dataloader retrieves one batch of data\n",
    "\n",
    "Upon successful completion of this section, you should be able to visualize four sample motion sequences by running `python dataloader.py`.\n",
    "\n",
    "Important notes:\n",
    "* There are multiple approaches to normalizing motion data. You are free to choose any method that produces reasonable training results in the subsequent sections.\n",
    "* Include visualizations of your **normalized motion** sequences in your submission for this part, following the example format below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62621af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./vid/sample_0.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video, display\n",
    "import glob\n",
    "\n",
    "for p in sorted(glob.glob(\"/content/Generative-Visual-Computing-3/motion-ae-output/part1_norm/*.mp4\")):\n",
    "    display(Video(p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e41d0c",
   "metadata": {},
   "source": [
    "## Part 2: Motion Manifold Learning (45 points)\n",
    "\n",
    "In this section, you will implement the neural network architecture and training methodology described in the reference paper to learn a motion manifold.\n",
    "\n",
    "### Convolutional Autoencoder Architecture (15 points)\n",
    "\n",
    "1. Implement the convolutional autoencoder following the architecture detailed in (one of) the reference paper in `MotionAutoencoder`.\n",
    "2. Develop both the encoding and decoding operations that will allow the network to compress motion data into a lower-dimensional manifold representation and then reconstruct it.\n",
    "\n",
    "### Training Procedure (30 points)\n",
    "\n",
    "1. Implement the training procedure in `MotionManifoldTrainer` for your autoencoder, carefully considering the appropriate loss function(s) to use for motion data.\n",
    "2. Generate and include training curves using the visualization template provided in the starter code.\n",
    "\n",
    "Important considerations:\n",
    "1. The reference paper was published several years ago when neural network implementations were often manually coded with custom operations. You'll need to adapt these concepts to modern PyTorch conventions and standard operations. Some training parameters may require adjustment to work effectively with contemporary deep learning frameworks.\n",
    "2. Your submission for this part should include clear visualizations of your training curves showing loss change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bc9fe",
   "metadata": {},
   "source": [
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "plot_path = os.path.join(\"/content/Generative-Visual-Computing-3/motion-ae-output/\", \"plots\", \"training_curves.png\")\n",
    "print(\"Plot exists:\", os.path.exists(plot_path), plot_path)\n",
    "display(Image(plot_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1865573",
   "metadata": {},
   "source": [
    "## Part 3: Motion Synthesis (30 points)\n",
    "\n",
    "### Motion Interpolation (15 points)\n",
    "\n",
    "Implement the function `MotionManifoldSynthesizer.interpolate_motions` that creates transitions between different motions using the learned manifold. This function should accept two motion sequences sampled from the dataset and generate an interpolated motion that blends naturally between them. You can visualize your results using the provided `visualize_interpolation` function.\n",
    "\n",
    "Your submission for this section should include at least two video examples demonstrating motion interpolation between different movement types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, glob\n",
    "from IPython.display import Video, HTML, display\n",
    "\n",
    "paths = [\"/content/Generative-Visual-Computing-3/motion-ae-output/interp_1.mp4\",\n",
    "         \"/content/Generative-Visual-Computing-3/motion-ae-output/interp_2.mp4\"]\n",
    "\n",
    "# 2) Display (embed=True is important in Colab)\n",
    "for p in paths:\n",
    "    if os.path.exists(p) and os.path.getsize(p) > 0:\n",
    "        display(Video(p, embed=True))\n",
    "    else:\n",
    "        print(f\"Skipping {p} (missing or empty).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c238d8",
   "metadata": {},
   "source": [
    "### Fixing Corrupt Motion Data (15 points)\n",
    "\n",
    "Implement the function `MotionManifoldSynthesizer.fix_corrupted_motion` that projects corrupted motion data onto the learned manifold and reconstructs corrected, natural-looking movements. This function should demonstrate the manifold's ability to act as a prior distribution over valid human motion. Use the provided `visualize_motion_comparison` function to create side-by-side comparisons of the corrupted input and your reconstructed output.\n",
    "\n",
    "Your submission for this section should include at least two video examples showing motion correction from different types of corruptions provided in the starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba45a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your videos of fixing corrupt motion data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb2200a",
   "metadata": {},
   "source": [
    "## Part 4. Analysis Questions (15 pt)\n",
    "\n",
    "Answer the question with your analysis. The questions are open-ended. We are looking for you own observasion from the expriments you did. Autoencoder is known as a relatively simple method so a lot of things here won't be perfect.\n",
    "\n",
    "1. Explain your chosen normalization approach for the motion data. Why did you select this method, and how does it specifically address the challenges of human motion data? What other normalization techniques did you consider, and why did you not choose them?\n",
    "\n",
    "[Answer]:\n",
    "I first removed the overall movement (the person’s forward drift), then normalized each joint by its own mean and standard deviation across the whole dataset. So i had to make everything “local,” then z-score per joint. I picked this because different joints move in different ranges, and so I wanted the model to focus on pose change, not where the person is in the room. I didn’t use per-clip normalization (it makes clips inconsistent with each other), min–max (touchy with outliers). This simple method was stable and worked well.\n",
    "\n",
    "2. After training your autoencoder, explore and describe the structure of your learned manifold. You can use t-SNE or PCA to visualize the hidden unit space (include one image in your answer).  Are different motion types clustered in particular regions? Can you identify meaningful directions in the latent space that correspond to specific motion attributes (speed, posture, etc.)?\n",
    "\n",
    "[Answer]:\n",
    "When I plotted the encoder features with PCA/t-SNE, similar motions appeared near each other. Walking, jogging, and running formed a continuum, a \"speed line\" from slow to fast. Another direction seemed to represent posture, from upright to leaned forward. Arm and upper-body actions appeared slightly off to the side. While the clusters weren’t perfect, distinct families of motion emerged with speed and posture being clear factors.\n",
    "\n",
    "3. Critically analyze the quality of your interpolated motions. Where does the interpolation succeed or fail? What patterns do you notice about transitions between dissimilar motions versus similar ones?\n",
    "\n",
    "[Answer]:\n",
    "Whne we work with blending between similar clips, the transition/blending is smooth. The feet, hips stayed mostly clean. However, in contrast, blending with very different clips introduced issues: like some foot sliding, or misaligned steps. SO it turns out that blending works best when the two clips are already similar and in sync, which makes sense while The farther apart they are, the more noticeable the glitches become.\n",
    "\n",
    "4. For the corrupted motion reconstruction task, analyze which types of corruption your system handles well versus poorly. What does this tell you about the properties of your learned manifold?\n",
    "\n",
    "[Answer]:\n",
    "The missing chunks or knocking out key joints (feet or hips) was harder as the model filled the gap with something average and it cause it to lose some style details, Which indicates that the model has a strong smooth and typical system, which is great for light noise but insufficient for large gaps or strict foot-contact rules. Adding contact awareness or stronger motion constraints would help with the tough cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba851f",
   "metadata": {},
   "source": [
    "## Extra Credit: Advanced Motion Synthesis and Editing (20 points)\n",
    "\n",
    "In this optional extra credit section, you'll implement more sophisticated motion synthesis techniques and potentially extend your model architecture to achieve these advanced tasks.\n",
    "\n",
    "For this task, you'll develop a method to complete partially specified motion sequences. Given a motion with missing frames, your system should intelligently fill in the gaps while maintaining natural movement characteristics and continuity.\n",
    "\n",
    "You need:\n",
    "- Develop a method to mask out and fill missing segments in motion sequences\n",
    "- Ensure smooth transitions between existing and synthesized motion\n",
    "- Leverage your trained motion manifold to generate plausible completions\n",
    "\n",
    "Your submission shoud:\n",
    "- Provide at least two examples of filling gaps in the middle of motion sequences\n",
    "- Provide at least two examples of extending incomplete motions by synthesizing the ending frames\n",
    "- For each visualization, you need to show the input and output side-by-side\n",
    "- Include a short analysis of your approach and the quality of your results\n",
    "\n",
    "### Motion Edtiting (10 points)\n",
    "\n",
    "For this task, you'll implement the style transfer technique described in \"A Deep Learning Framework for Character Motion Synthesis and Editing.\" This will allow you to transfer the style characteristics of one motion to another while preserving the content of the target motion.\n",
    "\n",
    "You need to develop the method to add constraint on the trained autoencoder.\n",
    "\n",
    "Your submission shoud:\n",
    "- Provide at least three examples of motion editing results between different motion types. Each visualization should include the original content motion, the reference motion, and your result\n",
    "- Write a short analysis of your results, discussing successes, limitations, and potential improvements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe4ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment & Repro ===\n",
    "import os, sys, json, random, platform, torch, numpy as np\n",
    "print(\"Python:\", sys.version.split()[0], \"| PyTorch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available())\n",
    "print(\"Platform:\", platform.platform())\n",
    "seed = 42\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Paths (edit DATA_DIR to your BVH root)\n",
    "DATA_DIR = \"./cmu-mocap\"      # <-- change me\n",
    "OUT_DIR  = \"./output/ae\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import CMUMotionDataset\n",
    "\n",
    "ds = CMUMotionDataset(\n",
    "    data_dir=DATA_DIR,\n",
    "    frame_rate=30,\n",
    "    window_size=160,\n",
    "    overlap=0.5,\n",
    "    include_velocity=True,\n",
    "    include_foot_contact=True\n",
    ")\n",
    "\n",
    "print(\"Windows:\", len(ds))\n",
    "print(\"Files:\", len(ds.motion_data))\n",
    "print(\"Joints:\", len(ds.get_joint_names()))\n",
    "print(\"Mean pose shape:\", ds.get_mean_pose().shape, \"| Std shape:\", ds.get_std().shape)\n",
    "\n",
    "s = ds[0]\n",
    "for k, v in s.items():\n",
    "    try:\n",
    "        print(f\"{k:28s}\", tuple(v.shape))\n",
    "    except:\n",
    "        print(f\"{k:28s}\", type(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c960360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AE import MotionManifoldTrainer\n",
    "\n",
    "trainer = MotionManifoldTrainer(\n",
    "    data_dir=DATA_DIR,\n",
    "    output_dir=OUT_DIR,\n",
    "    batch_size=16,\n",
    "    epochs=5,               # start with 5/5 to verify; raise to 25/25 for final\n",
    "    fine_tune_epochs=5,\n",
    "    learning_rate=1e-3,\n",
    "    fine_tune_lr=5e-4,\n",
    "    sparsity_weight=1e-3,\n",
    "    window_size=160,\n",
    "    val_split=0.1,\n",
    "    device=device\n",
    ")\n",
    "stats = trainer.train()\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import os, json\n",
    "\n",
    "plot_path = os.path.join(OUT_DIR, \"plots\", \"training_curves.png\")\n",
    "display(Image(filename=plot_path))\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"training_stats.json\"), \"r\") as f:\n",
    "    loaded_stats = json.load(f)\n",
    "loaded_stats.keys(), {k:(len(v[\"train_loss\"]), len(v[\"val_loss\"])) for k,v in loaded_stats.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Use the validation split that was created inside trainer\n",
    "val_loader = DataLoader(trainer.val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "def batch_to_input(batch, model_in_ch):\n",
    "    X = batch[\"positions_normalized_flat\"].to(device)        # [B,T,Cpos]\n",
    "    parts = [X]\n",
    "    if (\"trans_vel_xz\" in batch) and (\"rot_vel_y\" in batch):\n",
    "        tv = batch[\"trans_vel_xz\"].to(device)                 # [B,T,2]\n",
    "        ry = batch[\"rot_vel_y\"].to(device).unsqueeze(-1)      # [B,T,1]\n",
    "        parts += [tv, ry]\n",
    "    X_btC = torch.cat(parts, dim=-1)                          # [B,T,C]\n",
    "    X_bCt = X_btC.permute(0, 2, 1).contiguous()               # [B,C,T]\n",
    "    # pad/truncate to match model input channels\n",
    "    if X_bCt.size(1) < model_in_ch:\n",
    "        X_bCt = torch.cat([X_bCt, X_bCt.new_zeros(X_bCt.size(0), model_in_ch - X_bCt.size(1), X_bCt.size(2))], dim=1)\n",
    "    elif X_bCt.size(1) > model_in_ch:\n",
    "        X_bCt = X_bCt[:, :model_in_ch, :]\n",
    "    return X_bCt\n",
    "\n",
    "model = trainer.model.eval()\n",
    "in_ch = next(model.encoder.parameters()).shape[1]\n",
    "\n",
    "mse_list, mae_list, vel_mse_list = [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        x = batch_to_input(batch, in_ch)                        # [B,C,T]\n",
    "        x_hat, _ = model(x, corrupt_input=False)                # recon in normalized channel space\n",
    "\n",
    "        # only compare the position channels (first J*3)\n",
    "        J3 = batch[\"positions_normalized_flat\"].shape[-1]       # Cpos\n",
    "        x_pos  = x[:, :J3, :]                                   # [B, Cpos, T]\n",
    "        xh_pos = x_hat[:, :J3, :]                                # [B, Cpos, T]\n",
    "\n",
    "        mse = torch.mean((xh_pos - x_pos)**2).item()\n",
    "        mae = torch.mean(torch.abs(xh_pos - x_pos)).item()\n",
    "        # velocity mse in time\n",
    "        vel_x  = x_pos [:, :, 1:] - x_pos [:, :, :-1]\n",
    "        vel_xh = xh_pos[:, :, 1:] - xh_pos[:, :, :-1]\n",
    "        vel_mse = torch.mean((vel_xh - vel_x)**2).item()\n",
    "\n",
    "        mse_list.append(mse); mae_list.append(mae); vel_mse_list.append(vel_mse)\n",
    "\n",
    "print({\n",
    "    \"MSE (norm space)\": float(np.mean(mse_list)),\n",
    "    \"MAE (norm space)\": float(np.mean(mae_list)),\n",
    "    \"Velocity MSE\": float(np.mean(vel_mse_list))\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a31e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AE import MotionManifoldSynthesizer\n",
    "model_path = os.path.join(OUT_DIR, \"models\", \"motion_autoencoder.pt\")\n",
    "synth = MotionManifoldSynthesizer(model_path=model_path, dataset=trainer.dataset, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import visualize_interpolation\n",
    "ds = trainer.dataset\n",
    "m1 = ds[0]\n",
    "m2 = ds[min(1, len(ds)-1)]\n",
    "\n",
    "P_interp = synth.interpolate_motions(m1, m2, t=0.5)  # [1,T,J,3]\n",
    "interp_mp4 = os.path.join(OUT_DIR, \"interp.mp4\")\n",
    "visualize_interpolation(\n",
    "    m1[\"positions\"].unsqueeze(0).cpu(),\n",
    "    m2[\"positions\"].unsqueeze(0).cpu(),\n",
    "    P_interp.cpu(),\n",
    "    ds.joint_parents,\n",
    "    interp_mp4\n",
    ")\n",
    "interp_mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ef981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import visualize_motion_comparison\n",
    "sample = ds[0]\n",
    "corr, fixed = synth.fix_corrupted_motion(\n",
    "    sample,\n",
    "    corruption_type=\"zero\",\n",
    "    corruption_params={\"prob\": 0.5}\n",
    ")\n",
    "fix_mp4 = os.path.join(OUT_DIR, \"fix_compare.mp4\")\n",
    "visualize_motion_comparison(corr.cpu(), fixed.cpu(), ds.joint_parents, fix_mp4)\n",
    "fix_mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1513999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import visualize_motion_to_video\n",
    "for i in range(min(3, len(ds))):\n",
    "    out = os.path.join(OUT_DIR, f\"sample_{i}.mp4\")\n",
    "    visualize_motion_to_video(ds[i][\"positions\"], ds.joint_parents, out)\n",
    "out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
